{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469bf38d-e2cc-48f3-bb7e-964ac80e5670",
   "metadata": {},
   "source": [
    "# Image classification model using tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7074d-dabc-44a0-9a05-891af544000d",
   "metadata": {},
   "source": [
    "Deep neural network architecture with one input, one output, two hidden, and one dropout layer is used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631e9d55-266c-4b11-88fc-972c6dd86e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928746f4-1cec-43ff-b978-6f9f895bd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dropout': 0.25,\n",
    "    'batch-size': 128,\n",
    "    'epochs': 50,\n",
    "    'layer-1-size': 128,\n",
    "    'layer-2-size': 128,\n",
    "    'initial-lr': 0.01,\n",
    "    'decay-steps': 2000,\n",
    "    'decay-rate': 0.9,\n",
    "    'optimizer': 'adamax'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383e6778-900e-4a36-8091-ba2571b6e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading MNIST data\n",
    "mnist = tf.keras.datasets.mnist  \n",
    "num_class = 10\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25695960-651e-4846-88ea-2099da57f5a1",
   "metadata": {},
   "source": [
    "Need to reshape and normalize the training and test images, where normalization bounds image pixel intensity between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f66e37c-8c1f-4f60-b37a-566ee39d2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape and normalize the data\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\")/255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\")/255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_class)\n",
    "y_test = to_categorical(y_test, num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a731aed-d878-4732-89a1-910078a4fed6",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "1. Adding a Flatten layer to convert 2D image matrices to vectors. The input neurons correspond to the numbers in these vectors.\n",
    "2. Dense() method is used to add two hidden dense layers pulling in the hyperparameters from the “params” dictionary defined earlier with activation function as relu (Rectified Linear Unit).\n",
    "3. Add the dropout layer using the Dropout method. It is used to avoid overfitting while training the neural network.\n",
    "4. The output layer is the last layer in our network, which is defined using the Dense() method, corresponding to the number of classes.its)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f874bc54-9b50-4e2b-80c5-674a8cdb0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "# Get parameters from logged hyperparameters\n",
    "model = Sequential([\n",
    "  Flatten(input_shape=(784, )),\n",
    "  Dense(params['layer-1-size'], activation='relu'),\n",
    "  Dense(params['layer-2-size'], activation='relu'),\n",
    "  Dropout(params['dropout']),\n",
    "  Dense(10)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd70fd8f-cf62-4492-90a3-3ebd87b14bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=params['initial-lr'],\n",
    "    decay_steps=params['decay-steps'],\n",
    "    decay_rate=params['decay-rate']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4ab4d2-3863-4bf5-a584-a9602920cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adamax', \n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af6232f9-5485-4d57-b8d5-5b23ac19b5a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7642 - loss: 0.8231 - val_accuracy: 0.9324 - val_loss: 0.2318\n",
      "Epoch 2/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9262 - loss: 0.2556 - val_accuracy: 0.9500 - val_loss: 0.1711\n",
      "Epoch 3/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9446 - loss: 0.1890 - val_accuracy: 0.9571 - val_loss: 0.1429\n",
      "Epoch 4/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9543 - loss: 0.1538 - val_accuracy: 0.9616 - val_loss: 0.1225\n",
      "Epoch 5/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9611 - loss: 0.1313 - val_accuracy: 0.9648 - val_loss: 0.1096\n",
      "Epoch 6/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9659 - loss: 0.1141 - val_accuracy: 0.9688 - val_loss: 0.1003\n",
      "Epoch 7/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9703 - loss: 0.1003 - val_accuracy: 0.9711 - val_loss: 0.0942\n",
      "Epoch 8/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9735 - loss: 0.0910 - val_accuracy: 0.9735 - val_loss: 0.0869\n",
      "Epoch 9/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9759 - loss: 0.0809 - val_accuracy: 0.9745 - val_loss: 0.0828\n",
      "Epoch 10/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9778 - loss: 0.0751 - val_accuracy: 0.9750 - val_loss: 0.0796\n",
      "Epoch 11/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9789 - loss: 0.0703 - val_accuracy: 0.9764 - val_loss: 0.0758\n",
      "Epoch 12/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9821 - loss: 0.0613 - val_accuracy: 0.9772 - val_loss: 0.0746\n",
      "Epoch 13/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9826 - loss: 0.0575 - val_accuracy: 0.9774 - val_loss: 0.0731\n",
      "Epoch 14/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9848 - loss: 0.0515 - val_accuracy: 0.9783 - val_loss: 0.0717\n",
      "Epoch 15/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9864 - loss: 0.0489 - val_accuracy: 0.9784 - val_loss: 0.0690\n",
      "Epoch 16/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9864 - loss: 0.0462 - val_accuracy: 0.9787 - val_loss: 0.0695\n",
      "Epoch 17/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9868 - loss: 0.0437 - val_accuracy: 0.9779 - val_loss: 0.0724\n",
      "Epoch 18/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9879 - loss: 0.0408 - val_accuracy: 0.9801 - val_loss: 0.0668\n",
      "Epoch 19/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9880 - loss: 0.0396 - val_accuracy: 0.9783 - val_loss: 0.0710\n",
      "Epoch 20/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9898 - loss: 0.0355 - val_accuracy: 0.9807 - val_loss: 0.0665\n",
      "Epoch 21/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9901 - loss: 0.0337 - val_accuracy: 0.9804 - val_loss: 0.0654\n",
      "Epoch 22/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9904 - loss: 0.0325 - val_accuracy: 0.9794 - val_loss: 0.0672\n",
      "Epoch 23/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9905 - loss: 0.0325 - val_accuracy: 0.9805 - val_loss: 0.0664\n",
      "Epoch 24/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9917 - loss: 0.0280 - val_accuracy: 0.9803 - val_loss: 0.0682\n",
      "Epoch 25/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9922 - loss: 0.0265 - val_accuracy: 0.9799 - val_loss: 0.0667\n",
      "Epoch 26/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9925 - loss: 0.0250 - val_accuracy: 0.9810 - val_loss: 0.0661\n",
      "Epoch 27/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9930 - loss: 0.0238 - val_accuracy: 0.9814 - val_loss: 0.0650\n",
      "Epoch 28/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9942 - loss: 0.0210 - val_accuracy: 0.9815 - val_loss: 0.0650\n",
      "Epoch 29/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9948 - loss: 0.0203 - val_accuracy: 0.9811 - val_loss: 0.0659\n",
      "Epoch 30/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9941 - loss: 0.0201 - val_accuracy: 0.9818 - val_loss: 0.0652\n",
      "Epoch 31/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9948 - loss: 0.0183 - val_accuracy: 0.9811 - val_loss: 0.0662\n",
      "Epoch 32/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9952 - loss: 0.0168 - val_accuracy: 0.9807 - val_loss: 0.0667\n",
      "Epoch 33/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9953 - loss: 0.0167 - val_accuracy: 0.9814 - val_loss: 0.0680\n",
      "Epoch 34/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9953 - loss: 0.0160 - val_accuracy: 0.9819 - val_loss: 0.0704\n",
      "Epoch 35/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9959 - loss: 0.0157 - val_accuracy: 0.9822 - val_loss: 0.0700\n",
      "Epoch 36/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9964 - loss: 0.0138 - val_accuracy: 0.9828 - val_loss: 0.0675\n",
      "Epoch 37/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9968 - loss: 0.0124 - val_accuracy: 0.9814 - val_loss: 0.0692\n",
      "Epoch 38/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9969 - loss: 0.0116 - val_accuracy: 0.9815 - val_loss: 0.0687\n",
      "Epoch 39/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9966 - loss: 0.0125 - val_accuracy: 0.9821 - val_loss: 0.0688\n",
      "Epoch 40/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9971 - loss: 0.0114 - val_accuracy: 0.9823 - val_loss: 0.0691\n",
      "Epoch 41/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9973 - loss: 0.0101 - val_accuracy: 0.9818 - val_loss: 0.0729\n",
      "Epoch 42/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9975 - loss: 0.0104 - val_accuracy: 0.9817 - val_loss: 0.0722\n",
      "Epoch 43/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9973 - loss: 0.0099 - val_accuracy: 0.9821 - val_loss: 0.0719\n",
      "Epoch 44/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9973 - loss: 0.0098 - val_accuracy: 0.9818 - val_loss: 0.0721\n",
      "Epoch 45/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9978 - loss: 0.0085 - val_accuracy: 0.9816 - val_loss: 0.0735\n",
      "Epoch 46/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9977 - loss: 0.0087 - val_accuracy: 0.9809 - val_loss: 0.0750\n",
      "Epoch 47/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9981 - loss: 0.0080 - val_accuracy: 0.9818 - val_loss: 0.0734\n",
      "Epoch 48/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9979 - loss: 0.0086 - val_accuracy: 0.9806 - val_loss: 0.0802\n",
      "Epoch 49/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0071 - val_accuracy: 0.9813 - val_loss: 0.0758\n",
      "Epoch 50/50\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9983 - loss: 0.0075 - val_accuracy: 0.9810 - val_loss: 0.0777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1432445b310>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "    batch_size=params['batch-size'],\n",
    "    epochs=params['epochs'],\n",
    "    validation_data=(x_test, y_test),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "353e5c54-ced7-4e95-9da0-2c776a1000f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9784 - loss: 0.0921\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd7af5f-8b92-4a98-8ba3-42d72b9d25f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90c1cb-b72e-4229-8e46-77967e5d1c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ace1606-4ca3-4825-9c8e-2413e85ecea7",
   "metadata": {},
   "source": [
    "#### Tuning Hyperparameters in Neural Networks\n",
    "##### Learning Rate:  \r\n",
    "The learning rate tells the model how much to change based on its errors. If the learning rate is high, the model learns quickly but might make mistakes. If the learning rate is low, the model learns slowly but more carefully. This leads to less errors and better accuracy There are ways of adjusting the learning rate to achieve the best results possible. This involves adjusting the learning rate at predefined intervals during training. Furthermore, optimizers like the Adam enables a self-tuning of the learning rate according to the execution of the training.\n",
    "\n",
    "##### Batch Size:\n",
    "##### Number of Epochs:\n",
    "##### Activation Function:\n",
    "Common activation functions include ReLU, Sigmoid and Tanh.  \n",
    "ReLU makes the training of neural networks faster since it permits only the positive activations in neurons. Sigmoid is used for assigning probabilities since it outputs a value between 0 and 1. Tanh is advantageous especially when one does not want to use the whole scale which ranges from 0 to ± infinity. The selection of a right activation function requires careful consideration since it dictates whether the network shall be able to make a good prediction or not.\n",
    "##### Dropout:\r\n",
    "Dropout is a technique which is used to avoid overfitting of the model. It randomly deactivates or \"drops out\" some neurons by setting their outputs to zero during each training iteration. This process prevents neurons from relying too heavily on specific inputs, features, or other neurons. By discarding the result of specific neurons, dropout helps the network to focus on essential features in the process of training. Dropout is mostly implemented during training while it is disabled in the inference phase...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe55c38-6a97-4a88-91cd-f5097c6ea6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#providing grid for hyperparameter tunning\n",
    "'''param_grid = {\n",
    "    'initial-lr': [0.001, 0.01, 0.1],\n",
    "    'batch-size': [32, 64, 128]\n",
    "}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95a7e4-ba32-417c-9a1d-4f389bb0ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87653538-a1af-43c7-82bf-9f14cbc12502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
